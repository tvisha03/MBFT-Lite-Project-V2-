{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd5dc5f-dc75-424a-bf15-4d013501e612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898b12ca0ea4377a86e1dafaf2f5b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1332\n",
      "Validation size: 334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the unseen balanced dataset\n",
    "df_unseen = pd.read_csv('../data/unseen/client_ceas_unseen_balanced.csv')\n",
    "\n",
    "# Prepare the text data (combining subject and body)\n",
    "df_unseen['text'] = df_unseen['subject'].fillna('') + ' ' + df_unseen['body'].fillna('')\n",
    "\n",
    "# Convert the DataFrame to a HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df_unseen)\n",
    "\n",
    "# Load the tokenizer for TinyBERT\n",
    "MODEL_NAME = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Remove the 'text' column, as it's no longer needed\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "# If 'label' is present, rename to 'labels'\n",
    "if \"label\" in tokenized_dataset.column_names:\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set the dataset format to torch\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "dataset_dict = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "val_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Validation size:\", len(val_dataset))\n",
    "\n",
    "# You can now proceed with your model training or evaluation using `train_dataset` and `val_dataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe81a5d0-2b71-41d2-954b-acd82021e87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in tokenized dataset: 10\n",
      "Column names in tokenized dataset: ['sender', 'receiver', 'date', 'subject', 'body', 'labels', 'urls', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "{'sender': ['CNN Alerts <frakte_1973@emmeffe.net>', 'CNN Alerts <steffi-egnalnek@sorrentolactalis.com>', 'SpamExperts via Twitter <uiaregi@twitter.com>', '\"Astrology.com Daily Horoscope\" <dailyhoroscope@astrology.com>', 'CNN Alerts <lessomed@4wcz.tk>'], 'receiver': ['email151@gvc.ceas-challenge.cc', 'email775@gvc.ceas-challenge.cc', 'user2.1@gvc.ceas-challenge.cc', 'gvcormac@gvc.ceas-challenge.cc', 'user4@gvc.ceas-challenge.cc'], 'date': ['Fri, 08 Aug 2008 10:06:51 -0400', 'Fri, 08 Aug 2008 10:04:01 -0400', 'Fri, 08 Aug 2008 12:28:09 +0000', 'Fri, 08 Aug 2008 06:35:35 -0700', 'Fri, 08 Aug 2008 07:36:51 -0400'], 'subject': ['CNN Alerts: My Custom Alert', 'CNN Alerts: My Custom Alert', 'Direct message from SpamExperts via web', 'Astrology.com: Daily Horoscope', 'CNN Alerts: My Custom Alert'], 'body': ['\\n\\nCNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nAlert Name: My Custom Alert\\nShocking revelation about young girls in the US today.\\n\\nFri, 8 Aug 2008 16:31:38 +0300\\n\\nFULL STORY\\n\\n\\n\\nYou have agreed to receive this email from CNN.com as a result of your CNN.com preference settings.\\nTo manage your settings click here.\\nTo alter your alert criteria or frequency or to unsubscribe from receiving custom email alerts, click here.\\n\\n\\nCable News Network. One CNN Center, Atlanta, Georgia 30303\\n© 2008 Cable News Network.\\nA Time Warner Company\\nAll Rights Reserved.\\nView our privacy policy and terms.\\n\\n\\n\\n\\n\\n\\n', '\\n\\nCNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nAlert Name: My Custom Alert\\nBoys meet girls\\n\\nFri, 8 Aug 2008 10:28:47 -0300\\n\\nFULL STORY\\n\\n\\n\\nYou have agreed to receive this email from CNN.com as a result of your CNN.com preference settings.\\nTo manage your settings click here.\\nTo alter your alert criteria or frequency or to unsubscribe from receiving custom email alerts, click here.\\n\\n\\nCable News Network. One CNN Center, Atlanta, Georgia 30303\\n© 2008 Cable News Network.\\nA Time Warner Company\\nAll Rights Reserved.\\nView our privacy policy and terms.\\n\\n\\n\\n\\n\\n\\n', 'zz Memory lack of (10min) mt_se_xen3: ON\\n7.86 MB\\n\\nSpamExperts / spamexperts\\n\\n--\\nfollow me at http://twitter.com/spamexperts\\nreply on the web at http://twitter.com/direct_messages/create/spamexperts\\nsend me a direct message from your phone or IM: D SPAMEXPERTS your message here.\\nturn off these email notifications at: http://twitter.com/account/notifications\\n\\n\\n', \"\\n\\nAstrology.com Daily Horoscope\\n\\n\\n\\n\\nHoroscopes | \\nLove | \\n\\nPsychic Readings | Predictions | Numerology | Maya-Aztec | Tarot | Astrology Readings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDear Gordon,\\n              Here is your horoscope\\n              for Friday, August 8:\\n\\nYou can keep working and playing quite late tonight and should find that others can keep up with you only as long as you're around to motivate them. Use that great energy to start or finish something big!\\n\\nDo the signs point to love? Find out if you are truly compatible with a Free Psychic Reading. Call 800-648-1982.\\n\\n\\nCheck out your horoscopes for yesterday, \\n              tomorrow, \\n              or today's extended.\\n\\n\\n\\n\\n\\n\\nExtended\\nSingle's Love\\nCouple's Love\\n\\n\\nGreenScope\\nHome & Garden\\nWork\\n\\n\\nBeautyScope\\nAstroSlam\\nFlirt\\n\\n\\nCosmic Calendar\\nChinese\\nTeen\\n\\n\\nBabyScope\\nDogScope\\nCatScope\\n\\n\\nQuickie\\nLesbianScope\\nGayScope\\n\\n\\n\\xa0\\nEspañol\\n\\xa0\\n\\n\\n\\n\\n\\nP.S. Need advice? DearSugar will listen carefully, help appropriately and be your safe haven to ask questions and state your opinions about love and life.\\n\\n\\n\\nWarm Regards,\\n\\nhttp://www.astrology.com/\\nhttp://shop.astrology.com/\\n\\n\\n From Our Sponsor: Will a summer fling turn into the real thing? Find out with a Free Psychic Love Reading.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nRelationship Road Map\\n \\n \\nHow do you handle relationships in your life? Get a free sample Relationship Road Map reading.  \\n Click here to get started.\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\xa0\\nWatch On This Day for today's historic events and famous birthdays.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwatch this!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nBlabber: Sienna Miller Blows Her Top!\\n\\n\\n\\xa0\\nBat Signal Lights Up Gotham's Night\\n\\n\\n\\xa0\\nFamily Drama Smacks Down Brooke Hogan\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant a natural way to find happiness? Feel better and get in control of your\\nlife. Wendi's hypnosis program will get you looking forward to your future.  Learn more now!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAdditional Horoscopes\\nAries:\\xa0On most days, you're happy to engage with all comers, friend and foe alike. Today, though, you'd rather take time for yourself and think things through. It's a good day for self-reflection.\\nTaurus:\\xa0If you're coupled, your energy is probably devoted to your mate today. If you're single, you are almost certainly throwing yourself into the search -- or getting over your break-up. Things should move quickly.\\nGemini:\\xa0You're much more thoughtful than usual today and that could mean that you're finding it harder than ever to deal with immediate, pressing concerns. Try to put them off for a few days, if possible.\\nCancer:\\xa0Take all the time you need to make up your mind today -- or put off decisions until more information comes in. You need to know for sure that you're making the right call before your heart will be in it.\\nLeo:\\xa0It's a great time to pull back from the action and look more closely at the fundamentals of the situation, whatever it is. You can see nuance and detail that would elude almost anyone else.\\nVirgo:\\xa0Make sure that you take the time to hear everyone out today -- even if it feels as if it's a waste of time. The good you do by reinforcing their egos is plenty to balance the time lost.\\nLibra:\\xa0Work and money-related matters are foremost in your mind today and you should be able to tackle them with the gravity they deserve. Make sure there aren't any big surprises headed your way!\\nScorpio:\\xa0You can keep working and playing quite late tonight and should find that others can keep up with you only as long as you're around to motivate them. Use that great energy to start or finish something big!\\nSagittarius:\\xa0You're finding that your ambitions are changing somewhat -- though that does not mean they're softening! You should try to figure out what's beneath it all and then move forward toward the future.\\nCapricorn:\\xa0Look deeply within yourself and you should find some surprising ambitions. Clues have been popping up for quite a while, but they may all come together today and point you in a new direction.\\nAquarius:\\xa0You are focused on your career in a new way -- one that might surprise a few people! See if you can make that jump or lateral move with a minimum of fuss and drama, because you're eager to keep on track.\\nPisces:\\xa0It's too easy for you to let others have their way at your expense right now, so be on guard for exploitive behavior. If you know what to expect, you can shore up your own support so it evens out.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAstrology.com Special:\\nLook out for 'Number One' with a free sample Numerology Personality Profile.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nADVERTISEMENT\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFree Psychic Reading\\nWill you ever find Mr. Right? Find out with a Free Psychic Love Reading.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPregnancy & Baby Plus\\nPerfect for expecting & new moms -- The answers you need, all together & personalized for you.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSponsored Links from Yahoo!What's this?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClick here if you wish to edit your current information, add more signs to this email or sign up for more exciting newsletters.\\nClick here to unsubscribe from your daily horoscope.\\nIf you need to email us, please include the following lines in your reply.\\nemail recipient: gvcormac@gvc.ceas-challenge.cc\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n---\\nannmn:[743NOE043NOE52nr0F012001H0043NOE0mUb3aUbFT]\\n\\n\\n\\n\\n\", '\\n\\n\\nCNN Alerts: My Custom Alert\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nAlert Name: My Custom Alert\\nBaby borned with a moustache\\n\\nFri, 8 Aug 2008 13:34:59 +0200\\n\\nFULL STORY\\n\\n\\n\\nYou have agreed to receive this email from CNN.com as a result of your CNN.com preference settings.\\nTo manage your settings click here.\\nTo alter your alert criteria or frequency or to unsubscribe from receiving custom email alerts, click here.\\n\\n\\nCable News Network. One CNN Center, Atlanta, Georgia 30303\\n© 2008 Cable News Network.\\nA Time Warner Company\\nAll Rights Reserved.\\nView our privacy policy and terms.\\n\\n\\n\\n\\n\\n\\n'], 'labels': tensor([1, 1, 0, 0, 1]), 'urls': tensor([0, 0, 1, 1, 0]), 'input_ids': tensor([[  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499, 16880, 11449,  2055,  2402,  3057,  1999,  1996,  2149,  2651,\n",
      "          1012, 10424,  2072,  1010,  1022, 15476,  2263,  2385,  1024,  2861,\n",
      "          1024,  4229,  1009,  6021,  8889,  2440,  2466,  2017,  2031,  3530,\n",
      "          2000,  4374,  2023, 10373,  2013, 13229,  1012,  4012,  2004,  1037,\n",
      "          2765,  1997,  2115, 13229,  1012,  4012, 12157, 10906,  1012,  2000,\n",
      "          6133,  2115, 10906, 11562,  2182,  1012,  2000, 11477,  2115,  9499,\n",
      "          9181,  2030,  6075,  2030,  2000,  4895,  6342,  5910, 26775, 20755,\n",
      "          2013,  4909,  7661, 10373,  9499,  2015,  1010, 11562,  2182,  1012,\n",
      "          5830,  2739,  2897,  1012,  2028, 13229,  2415,  1010,  5865,  1010,\n",
      "          4108, 19988,  2692,  2509,  1075,  2263,  5830,  2739,  2897,  1012,\n",
      "          1037,  2051,  6654,  2194,  2035,  2916,  9235,   102],\n",
      "        [  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499,  3337,  3113,  3057, 10424,  2072,  1010,  1022, 15476,  2263,\n",
      "          2184,  1024,  2654,  1024,  4700,  1011,  6021,  8889,  2440,  2466,\n",
      "          2017,  2031,  3530,  2000,  4374,  2023, 10373,  2013, 13229,  1012,\n",
      "          4012,  2004,  1037,  2765,  1997,  2115, 13229,  1012,  4012, 12157,\n",
      "         10906,  1012,  2000,  6133,  2115, 10906, 11562,  2182,  1012,  2000,\n",
      "         11477,  2115,  9499,  9181,  2030,  6075,  2030,  2000,  4895,  6342,\n",
      "          5910, 26775, 20755,  2013,  4909,  7661, 10373,  9499,  2015,  1010,\n",
      "         11562,  2182,  1012,  5830,  2739,  2897,  1012,  2028, 13229,  2415,\n",
      "          1010,  5865,  1010,  4108, 19988,  2692,  2509,  1075,  2263,  5830,\n",
      "          2739,  2897,  1012,  1037,  2051,  6654,  2194,  2035,  2916,  9235,\n",
      "          1012,  3193,  2256,  9394,  3343,  1998,  3408,   102],\n",
      "        [  101,  3622,  4471,  2013, 12403,  4168,  2595,  4842,  3215,  3081,\n",
      "          4773,  1062,  2480,  3638,  3768,  1997,  1006,  2184, 10020,  1007,\n",
      "         11047,  1035,  7367,  1035,  1060,  2368,  2509,  1024,  2006,  1021,\n",
      "          1012,  6564, 16914, 12403,  4168,  2595,  4842,  3215,  1013, 12403,\n",
      "          4168,  2595,  4842,  3215,  1011,  1011,  3582,  2033,  2012,  8299,\n",
      "          1024,  1013,  1013, 10474,  1012,  4012,  1013, 12403,  4168,  2595,\n",
      "          4842,  3215,  7514,  2006,  1996,  4773,  2012,  8299,  1024,  1013,\n",
      "          1013, 10474,  1012,  4012,  1013,  3622,  1035,  7696,  1013,  3443,\n",
      "          1013, 12403,  4168,  2595,  4842,  3215,  4604,  2033,  1037,  3622,\n",
      "          4471,  2013,  2115,  3042,  2030, 10047,  1024,  1040, 12403,  4168,\n",
      "          2595,  4842,  3215,  2115,  4471,  2182,  1012,  2735,  2125,  2122,\n",
      "         10373, 26828,  2015,  2012,  1024,  8299,  1024,  1013,  1013, 10474,\n",
      "          1012,  4012,  1013,  4070,  1013, 26828,  2015,   102],\n",
      "        [  101, 28625,  6483,  1012,  4012,  1024,  3679,  7570,  7352, 16186,\n",
      "         28625,  6483,  1012,  4012,  3679,  7570,  7352, 16186,  7570,  7352,\n",
      "         16186,  2015,  1064,  2293,  1064, 12663, 15324,  1064, 20932,  1064,\n",
      "         16371,  5017,  6779,  1064,  9815,  1011, 25245,  1064, 16985,  4140,\n",
      "          1064, 28625,  6483, 15324,  6203,  5146,  1010,  2182,  2003,  2115,\n",
      "          7570,  7352, 16186,  2005,  5958,  1010,  2257,  1022,  1024,  2017,\n",
      "          2064,  2562,  2551,  1998,  2652,  3243,  2397,  3892,  1998,  2323,\n",
      "          2424,  2008,  2500,  2064,  2562,  2039,  2007,  2017,  2069,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  2105,  2000,  9587, 29068,  3686,\n",
      "          2068,  1012,  2224,  2008,  2307,  2943,  2000,  2707,  2030,  3926,\n",
      "          2242,  2502,   999,  2079,  1996,  5751,  2391,  2000,  2293,  1029,\n",
      "          2424,  2041,  2065,  2017,  2024,  5621, 11892,  2007,  1037,  2489,\n",
      "         12663,  3752,  1012,  2655,  5385,  1011,  4185,   102],\n",
      "        [  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499,  3336, 15356,  2094,  2007,  1037,  9587, 19966, 15395, 10424,\n",
      "          2072,  1010,  1022, 15476,  2263,  2410,  1024,  4090,  1024,  5354,\n",
      "          1009,  6185,  8889,  2440,  2466,  2017,  2031,  3530,  2000,  4374,\n",
      "          2023, 10373,  2013, 13229,  1012,  4012,  2004,  1037,  2765,  1997,\n",
      "          2115, 13229,  1012,  4012, 12157, 10906,  1012,  2000,  6133,  2115,\n",
      "         10906, 11562,  2182,  1012,  2000, 11477,  2115,  9499,  9181,  2030,\n",
      "          6075,  2030,  2000,  4895,  6342,  5910, 26775, 20755,  2013,  4909,\n",
      "          7661, 10373,  9499,  2015,  1010, 11562,  2182,  1012,  5830,  2739,\n",
      "          2897,  1012,  2028, 13229,  2415,  1010,  5865,  1010,  4108, 19988,\n",
      "          2692,  2509,  1075,  2263,  5830,  2739,  2897,  1012,  1037,  2051,\n",
      "          6654,  2194,  2035,  2916,  9235,  1012,  3193,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "✅ Tokenized dataset saved to ../data/unseen/tokenized_client_ceas_unseen.csv\n"
     ]
    }
   ],
   "source": [
    "# Print the number of columns in the tokenized dataset\n",
    "print(f\"Number of columns in tokenized dataset: {len(tokenized_dataset.column_names)}\")\n",
    "\n",
    "# Print the column names to inspect the tokenized dataset\n",
    "print(f\"Column names in tokenized dataset: {tokenized_dataset.column_names}\")\n",
    "\n",
    "# Print a preview of the tokenized dataset (first few rows)\n",
    "print(tokenized_dataset[:5])  # Prints the first 5 entries of the dataset\n",
    "\n",
    "# If you want to save the tokenized dataset to disk as CSV (or another format), you can do it like this:\n",
    "\n",
    "# Convert the tokenized dataset to a pandas DataFrame for saving\n",
    "tokenized_df = pd.DataFrame(tokenized_dataset)\n",
    "\n",
    "# Save to CSV (you can change the path)\n",
    "save_path = '../data/unseen/tokenized_client_ceas_unseen.csv'\n",
    "tokenized_df.to_csv(save_path, index=False)\n",
    "print(f\"✅ Tokenized dataset saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf6ab9e-bc61-4238-ae5a-c2e3607e4f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef1847c1ccb4d46a6767be4ad583f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned and saved tokenized dataset to ../data/unseen/cleaned_tokenized_client_ceas_unseen.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "# Remove columns that are not needed\n",
    "df_cleaned = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col not in columns_to_keep])\n",
    "\n",
    "# Save the cleaned dataset to a CSV\n",
    "cleaned_save_path = \"../data/unseen/cleaned_tokenized_client_ceas_unseen.csv\"\n",
    "df_cleaned.to_csv(cleaned_save_path, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned and saved tokenized dataset to {cleaned_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f82b63-803a-4d9a-be2a-6de21d6c7bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1, 0, 0, 1]), 'input_ids': tensor([[  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499, 16880, 11449,  2055,  2402,  3057,  1999,  1996,  2149,  2651,\n",
      "          1012, 10424,  2072,  1010,  1022, 15476,  2263,  2385,  1024,  2861,\n",
      "          1024,  4229,  1009,  6021,  8889,  2440,  2466,  2017,  2031,  3530,\n",
      "          2000,  4374,  2023, 10373,  2013, 13229,  1012,  4012,  2004,  1037,\n",
      "          2765,  1997,  2115, 13229,  1012,  4012, 12157, 10906,  1012,  2000,\n",
      "          6133,  2115, 10906, 11562,  2182,  1012,  2000, 11477,  2115,  9499,\n",
      "          9181,  2030,  6075,  2030,  2000,  4895,  6342,  5910, 26775, 20755,\n",
      "          2013,  4909,  7661, 10373,  9499,  2015,  1010, 11562,  2182,  1012,\n",
      "          5830,  2739,  2897,  1012,  2028, 13229,  2415,  1010,  5865,  1010,\n",
      "          4108, 19988,  2692,  2509,  1075,  2263,  5830,  2739,  2897,  1012,\n",
      "          1037,  2051,  6654,  2194,  2035,  2916,  9235,   102],\n",
      "        [  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499,  3337,  3113,  3057, 10424,  2072,  1010,  1022, 15476,  2263,\n",
      "          2184,  1024,  2654,  1024,  4700,  1011,  6021,  8889,  2440,  2466,\n",
      "          2017,  2031,  3530,  2000,  4374,  2023, 10373,  2013, 13229,  1012,\n",
      "          4012,  2004,  1037,  2765,  1997,  2115, 13229,  1012,  4012, 12157,\n",
      "         10906,  1012,  2000,  6133,  2115, 10906, 11562,  2182,  1012,  2000,\n",
      "         11477,  2115,  9499,  9181,  2030,  6075,  2030,  2000,  4895,  6342,\n",
      "          5910, 26775, 20755,  2013,  4909,  7661, 10373,  9499,  2015,  1010,\n",
      "         11562,  2182,  1012,  5830,  2739,  2897,  1012,  2028, 13229,  2415,\n",
      "          1010,  5865,  1010,  4108, 19988,  2692,  2509,  1075,  2263,  5830,\n",
      "          2739,  2897,  1012,  1037,  2051,  6654,  2194,  2035,  2916,  9235,\n",
      "          1012,  3193,  2256,  9394,  3343,  1998,  3408,   102],\n",
      "        [  101,  3622,  4471,  2013, 12403,  4168,  2595,  4842,  3215,  3081,\n",
      "          4773,  1062,  2480,  3638,  3768,  1997,  1006,  2184, 10020,  1007,\n",
      "         11047,  1035,  7367,  1035,  1060,  2368,  2509,  1024,  2006,  1021,\n",
      "          1012,  6564, 16914, 12403,  4168,  2595,  4842,  3215,  1013, 12403,\n",
      "          4168,  2595,  4842,  3215,  1011,  1011,  3582,  2033,  2012,  8299,\n",
      "          1024,  1013,  1013, 10474,  1012,  4012,  1013, 12403,  4168,  2595,\n",
      "          4842,  3215,  7514,  2006,  1996,  4773,  2012,  8299,  1024,  1013,\n",
      "          1013, 10474,  1012,  4012,  1013,  3622,  1035,  7696,  1013,  3443,\n",
      "          1013, 12403,  4168,  2595,  4842,  3215,  4604,  2033,  1037,  3622,\n",
      "          4471,  2013,  2115,  3042,  2030, 10047,  1024,  1040, 12403,  4168,\n",
      "          2595,  4842,  3215,  2115,  4471,  2182,  1012,  2735,  2125,  2122,\n",
      "         10373, 26828,  2015,  2012,  1024,  8299,  1024,  1013,  1013, 10474,\n",
      "          1012,  4012,  1013,  4070,  1013, 26828,  2015,   102],\n",
      "        [  101, 28625,  6483,  1012,  4012,  1024,  3679,  7570,  7352, 16186,\n",
      "         28625,  6483,  1012,  4012,  3679,  7570,  7352, 16186,  7570,  7352,\n",
      "         16186,  2015,  1064,  2293,  1064, 12663, 15324,  1064, 20932,  1064,\n",
      "         16371,  5017,  6779,  1064,  9815,  1011, 25245,  1064, 16985,  4140,\n",
      "          1064, 28625,  6483, 15324,  6203,  5146,  1010,  2182,  2003,  2115,\n",
      "          7570,  7352, 16186,  2005,  5958,  1010,  2257,  1022,  1024,  2017,\n",
      "          2064,  2562,  2551,  1998,  2652,  3243,  2397,  3892,  1998,  2323,\n",
      "          2424,  2008,  2500,  2064,  2562,  2039,  2007,  2017,  2069,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  2105,  2000,  9587, 29068,  3686,\n",
      "          2068,  1012,  2224,  2008,  2307,  2943,  2000,  2707,  2030,  3926,\n",
      "          2242,  2502,   999,  2079,  1996,  5751,  2391,  2000,  2293,  1029,\n",
      "          2424,  2041,  2065,  2017,  2024,  5621, 11892,  2007,  1037,  2489,\n",
      "         12663,  3752,  1012,  2655,  5385,  1011,  4185,   102],\n",
      "        [  101, 13229,  9499,  2015,  1024,  2026,  7661,  9499, 13229,  9499,\n",
      "          2015,  1024,  2026,  7661,  9499,  9499,  2171,  1024,  2026,  7661,\n",
      "          9499,  3336, 15356,  2094,  2007,  1037,  9587, 19966, 15395, 10424,\n",
      "          2072,  1010,  1022, 15476,  2263,  2410,  1024,  4090,  1024,  5354,\n",
      "          1009,  6185,  8889,  2440,  2466,  2017,  2031,  3530,  2000,  4374,\n",
      "          2023, 10373,  2013, 13229,  1012,  4012,  2004,  1037,  2765,  1997,\n",
      "          2115, 13229,  1012,  4012, 12157, 10906,  1012,  2000,  6133,  2115,\n",
      "         10906, 11562,  2182,  1012,  2000, 11477,  2115,  9499,  9181,  2030,\n",
      "          6075,  2030,  2000,  4895,  6342,  5910, 26775, 20755,  2013,  4909,\n",
      "          7661, 10373,  9499,  2015,  1010, 11562,  2182,  1012,  5830,  2739,\n",
      "          2897,  1012,  2028, 13229,  2415,  1010,  5865,  1010,  4108, 19988,\n",
      "          2692,  2509,  1075,  2263,  5830,  2739,  2897,  1012,  1037,  2051,\n",
      "          6654,  2194,  2035,  2916,  9235,  1012,  3193,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8977cca4-e754-4074-920b-bbdc2189265a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/mbft_lite_env/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 00:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.593911</td>\n",
       "      <td>0.952096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.299900</td>\n",
       "      <td>0.229219</td>\n",
       "      <td>0.958084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.092338</td>\n",
       "      <td>0.979042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.09233815968036652, 'eval_accuracy': 0.9790419161676647, 'eval_runtime': 0.6168, 'eval_samples_per_second': 541.526, 'eval_steps_per_second': 9.728, 'epoch': 3.0}\n",
      "✅ Probabilities saved to ../data/unseen/tinybert_probabilities.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the model and tokenizer\n",
    "MODEL_NAME = \"huawei-noah/TinyBERT_General_4L_312D\"  # TinyBERT model\n",
    "num_labels = 2  # binary classification (phishing vs legit)\n",
    "\n",
    "# Load the pre-trained TinyBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinybert_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=10  # Logs every 10 steps\n",
    ")\n",
    "\n",
    "\n",
    "# Define the metric function for accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Training dataset (already tokenized)\n",
    "    eval_dataset=val_dataset,  # Validation dataset (already tokenized)\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results (accuracy)\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Generate probabilities on the validation dataset\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions and probabilities\n",
    "import torch\n",
    "\n",
    "def get_probabilities(dataset):\n",
    "    # Get the logits (raw predictions) from the model\n",
    "    logits = trainer.predict(dataset).predictions\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "    return probabilities\n",
    "\n",
    "# Get probabilities for the validation set\n",
    "probabilities = get_probabilities(val_dataset)\n",
    "\n",
    "# Save the probabilities to a CSV file\n",
    "probabilities_df = pd.DataFrame(probabilities, columns=[\"prob_class_0\", \"prob_class_1\"])\n",
    "probabilities_df[\"labels\"] = val_dataset[\"labels\"]\n",
    "\n",
    "# Save the probabilities to CSV\n",
    "probabilities_save_path = \"../data/unseen/tinybert_probabilities.csv\"\n",
    "probabilities_df.to_csv(probabilities_save_path, index=False)\n",
    "\n",
    "print(f\"✅ Probabilities saved to {probabilities_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea607b04-31cb-4775-a2dd-6f5bda0b75b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
