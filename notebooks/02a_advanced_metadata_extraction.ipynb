{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928ced5e-d75d-4873-bed6-1ea9ffcf0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "from dateutil.parser import UnknownTimezoneWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UnknownTimezoneWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d8b03d-982f-451f-b56d-079c5c9d517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(email):\n",
    "    try:\n",
    "        return email.split(\"@\")[-1].lower()\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def is_public_domain(domain):\n",
    "    public_domains = {\"gmail.com\", \"yahoo.com\", \"outlook.com\", \"hotmail.com\", \"aol.com\", \"protonmail.com\"}\n",
    "    return int(domain in public_domains)\n",
    "\n",
    "def count_suspicious_chars(s):\n",
    "    return sum(s.count(c) for c in ['-', '_', '~', '%', '='])\n",
    "\n",
    "def has_digits(s):\n",
    "    return int(any(char.isdigit() for char in s))\n",
    "\n",
    "def get_tld(domain):\n",
    "    parts = domain.split(\".\")\n",
    "    return parts[-1] if len(parts) > 1 else \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda7b28a-7464-4924-b0e7-057a3c53317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url_features(text):\n",
    "    url_pattern = r\"(https?://[^\\s]+)\"\n",
    "    urls = re.findall(url_pattern, str(text))\n",
    "    url_count = len(urls)\n",
    "    url_lengths = [len(u) for u in urls]\n",
    "    avg_length = np.mean(url_lengths) if url_lengths else 0\n",
    "    max_length = np.max(url_lengths) if url_lengths else 0\n",
    "    has_ip = int(any(re.search(r\"http[s]?://(?:\\d{1,3}\\.){3}\\d{1,3}\", u) for u in urls))\n",
    "    has_special_chars = sum(u.count('?') + u.count('&') + u.count('=') + u.count('@') + u.count('%') for u in urls)\n",
    "    has_redirect = int(any(u.count(\"//\") > 1 or u.count(\"http\") > 1 for u in urls))\n",
    "\n",
    "    suspicious_tlds = {\"tk\", \"xyz\", \"ru\", \"top\", \"ml\", \"ga\", \"cf\", \"gq\"}\n",
    "    tlds = []\n",
    "    for u in urls:\n",
    "        try:\n",
    "            parsed = urlparse(u)\n",
    "            domain = parsed.netloc\n",
    "            if domain:\n",
    "                tlds.append(get_tld(domain))\n",
    "        except:\n",
    "            continue\n",
    "    has_suspicious_tld = int(any(tld in suspicious_tlds for tld in tlds))\n",
    "    return pd.Series([url_count, avg_length, max_length, has_ip, has_special_chars, has_redirect, has_suspicious_tld])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583493d1-a254-4778-91fa-85d68ada6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metadata_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"sender_domain\"] = df[\"sender\"].apply(extract_domain)\n",
    "    df[\"receiver_domain\"] = df[\"receiver\"].apply(extract_domain)\n",
    "\n",
    "    df[\"sender_domain_length\"] = df[\"sender_domain\"].apply(len)\n",
    "    df[\"sender_has_digits\"] = df[\"sender_domain\"].apply(has_digits)\n",
    "    df[\"sender_has_special_chars\"] = df[\"sender_domain\"].apply(count_suspicious_chars)\n",
    "    df[\"sender_tld\"] = df[\"sender_domain\"].apply(get_tld)\n",
    "    df[\"sender_is_public_domain\"] = df[\"sender_domain\"].apply(is_public_domain)\n",
    "    df[\"receiver_is_undisclosed\"] = df[\"receiver\"].fillna(\"\").str.contains(\"undisclosed\", case=False).astype(int)\n",
    "    df[\"receiver_is_public_domain\"] = df[\"receiver_domain\"].apply(is_public_domain)\n",
    "    df[\"sender_equals_receiver\"] = (df[\"sender_domain\"] == df[\"receiver_domain\"]).astype(int)\n",
    "\n",
    "    # Date-based features\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', utc=True)\n",
    "    df[\"email_hour\"] = df[\"date\"].dt.hour.fillna(-1).astype(int)\n",
    "    df[\"email_weekday\"] = df[\"date\"].dt.weekday.fillna(-1).astype(int)\n",
    "    df[\"is_weekend\"] = df[\"email_weekday\"].isin([5, 6]).astype(int)\n",
    "    df[\"is_midnight_hour\"] = df[\"email_hour\"].isin(range(0, 6)).astype(int)\n",
    "    df[\"has_valid_date\"] = df[\"email_hour\"].apply(lambda x: 1 if x != -1 else 0)\n",
    "\n",
    "    # URL-based features from body\n",
    "    url_feats = df[\"body\"].apply(extract_url_features)\n",
    "    url_feats.columns = [\"url_count_in_body\", \"url_avg_length\", \"url_max_length\",\n",
    "                         \"url_has_ip\", \"url_has_special_chars\", \"url_has_redirect\", \"url_suspicious_tld\"]\n",
    "    df = pd.concat([df, url_feats], axis=1)\n",
    "\n",
    "    # Text statistics\n",
    "    df[\"subject_length\"] = df[\"subject\"].fillna(\"\").apply(len)\n",
    "    df[\"body_length\"] = df[\"body\"].fillna(\"\").apply(len)\n",
    "    df[\"text_combined_length\"] = df[\"subject_length\"] + df[\"body_length\"]\n",
    "    df[\"uppercase_ratio\"] = df[\"body\"].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / len(str(x)) if len(str(x)) > 0 else 0)\n",
    "    df[\"exclamation_count\"] = df[\"body\"].fillna(\"\").str.count(\"!\")\n",
    "\n",
    "    df[\"url_present\"] = df[\"urls\"]\n",
    "    df[\"label\"] = df[\"label\"]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df[\"sender_tld\"] = le.fit_transform(df[\"sender_tld\"].astype(str))\n",
    "\n",
    "    selected_columns = [\n",
    "        \"sender_domain_length\", \"sender_has_digits\", \"sender_has_special_chars\",\n",
    "        \"sender_tld\", \"sender_is_public_domain\", \"receiver_is_undisclosed\",\n",
    "        \"receiver_is_public_domain\", \"sender_equals_receiver\", \"email_hour\",\n",
    "        \"email_weekday\", \"is_weekend\", \"is_midnight_hour\", \"has_valid_date\",  # ✅ NEW\n",
    "        \"url_present\", \"url_count_in_body\", \"url_avg_length\", \"url_max_length\",\n",
    "        \"url_has_ip\", \"url_has_special_chars\", \"url_has_redirect\", \"url_suspicious_tld\",\n",
    "        \"subject_length\", \"body_length\", \"text_combined_length\", \"uppercase_ratio\",\n",
    "        \"exclamation_count\", \"label\"\n",
    "    ]\n",
    "    return df[selected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6819ac6b-2034-4d40-a243-8737c327a947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved enhanced metadata for client_1 → ../data/processed/metadata_v2/client_1_meta_v2.csv\n",
      "✅ Saved enhanced metadata for client_2 → ../data/processed/metadata_v2/client_2_meta_v2.csv\n",
      "✅ Saved enhanced metadata for client_3 → ../data/processed/metadata_v2/client_3_meta_v2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/4d6dk50x4cjgbl61mz0_173c0000gn/T/ipykernel_31776/3415018170.py:16: FutureWarning: Parsed string \"Wed, 18 Sep 2002 11:43:02 PST\" included an un-recognized timezone \"PST\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', utc=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved enhanced metadata for client_4 → ../data/processed/metadata_v2/client_4_meta_v2.csv\n"
     ]
    }
   ],
   "source": [
    "client_paths = {\n",
    "    \"client_1\": \"../data/clients/client_1.csv\",\n",
    "    \"client_2\": \"../data/clients/client_2.csv\",\n",
    "    \"client_3\": \"../data/clients/client_3.csv\",\n",
    "    \"client_4\": \"../data/clients/client_4.csv\"\n",
    "}\n",
    "\n",
    "output_dir = \"../data/processed/metadata_v2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for client, path in client_paths.items():\n",
    "    df = pd.read_csv(path)\n",
    "    processed = build_metadata_features(df)\n",
    "    out_path = os.path.join(output_dir, f\"{client}_meta_v2.csv\")\n",
    "    processed.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Saved enhanced metadata for {client} → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2f2d7-a32b-4aa8-b26c-2eb1670cd46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
